{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as pl\n",
    "from os.path import join, split, splitext\n",
    "from pypllon.parsers import parse_ctx, load_complex_array\n",
    "from glob import glob\n",
    "from tools.plot import imshow\n",
    "\n",
    "import os\n",
    "try:\n",
    "    ID = os.environ['ID']\n",
    "    BASEDIR = os.environ['BASEDIR']\n",
    "    BASEDIR_OFFSET = os.environ.get('BASEDIR_OFFSET', None)\n",
    "except KeyError:\n",
    "    ID = 'M5Fou'\n",
    "    BASEDIR = '/Users/dsuess/Downloads/Phaselift_2017_8_4/5x5 phaselift/'\n",
    "    BASEDIR_OFFSET = '/Users/dsuess/Downloads/Phaselift_2017_14_4/5x5 phaselift/'\n",
    "   \n",
    "print(ID, '\\n', BASEDIR,'\\n', BASEDIR_OFFSET)\n",
    "DIM = int(ID[1])\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# labels of the detectors used\n",
    "DETECTORS_ALL = ['AH', 'BH', 'CH', 'DH', 'EH', 'FH']\n",
    "DETECTORS = DETECTORS_ALL[1:1 + DIM]\n",
    "# id of the experiment\n",
    "f = h5py.File('{}.h5'.format(ID), 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tools.helpers import get_git_revision_hash\n",
    "\n",
    "f.attrs['COMMITID'] = get_git_revision_hash()\n",
    "f.attrs['DETECTORS'] = [s.encode() for s in DETECTORS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preparation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The matrix to be recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gauss or RECR doesn't matter for this\n",
    "targetfile = BASEDIR + '/Unitaries/%s.dat' % ID\n",
    "target = load_complex_array(targetfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pl.subplot(131)\n",
    "imshow(target.real, interpolation='nearest', show=False)\n",
    "pl.subplot(132)\n",
    "imshow(target.imag, interpolation='nearest', show=False)\n",
    "pl.subplot(133)\n",
    "imshow(np.abs(target), interpolation='nearest')\n",
    "pl.show()\n",
    "f['TARGET'] = target\n",
    "f['TARGETFILE'] = targetfile\n",
    "f.flush()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvecsgroup = f.create_group('INVECS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_pvecs(fnames, prepvecs):\n",
    "    for fname in fnames:\n",
    "        vecid = split(splitext(fname)[0])[1]\n",
    "        prepvecs[vecid] = load_complex_array(fname)\n",
    "        prepvecs[vecid].attrs['FILE'] = fname\n",
    "\n",
    "    f.flush()\n",
    "    \n",
    "fnames = glob(BASEDIR + '/Vectors/V%i_*.dat' % DIM)\n",
    "prepvecs = pvecsgroup.create_group('GAUSS')\n",
    "print(\"Number of Gaussian preparation vectors: %i\" % len(fnames))\n",
    "load_pvecs(fnames, prepvecs)\n",
    "\n",
    "fnames = glob(BASEDIR + '/Vectors/VRad%i_*.dat' % DIM)\n",
    "prepvecs = pvecsgroup.create_group('RECR')\n",
    "print(\"Number of RECR preparation vectors: %i\" % len(fnames))\n",
    "load_pvecs(fnames, prepvecs)\n",
    "\n",
    "if BASEDIR_OFFSET is not None:\n",
    "    fnames = glob(BASEDIR_OFFSET + '/Vectors_offset/VRad%i_offset_*.dat' % DIM)\n",
    "    prepvecs = pvecsgroup.create_group('RECR_OFFSET')\n",
    "    print(\"Number of RECR_OFFSET preparation vectors: %i\" % len(fnames))\n",
    "    load_pvecs(fnames, prepvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phaselift Raw Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_deteff(fname):\n",
    "    summarydict = parse_ctx(fname)\n",
    "    return  np.array([summarydict['det_eff'][det] for det in DETECTORS])\n",
    "\n",
    "def dict_to_rates(dic):\n",
    "    return np.array([dic.get(det.lower(), 0) for det in DETECTORS])\n",
    "\n",
    "def load_counts(fname):\n",
    "    summarydict = parse_ctx(fname)\n",
    "    rates = np.array([summarydict[det] for det in DETECTORS])\n",
    "\n",
    "    parent = summarydict['metadata']['parent']\n",
    "    path_to_raw = join(split(fname)[0], '..', 'raw', parent + '.ctx')\n",
    "    \n",
    "    try:\n",
    "        c = parse_ctx(path_to_raw)\n",
    "        raw_rates = np.array([dict_to_rates(val) \n",
    "                              for key, val in c.items()\n",
    "                              if key.startswith('count_rates')])\n",
    "\n",
    "        assert np.all(np.sum(raw_rates, axis=0) == rates)\n",
    "        return raw_rates, path_to_raw\n",
    "    except FileNotFoundError:\n",
    "        return rates[None, :], fname\n",
    "\n",
    "def vector_to_counts(globpatt):\n",
    "    matches = glob(globpatt)\n",
    "    if len(matches) != 1:\n",
    "        raise IOError(\"Wrong number of matches %i\" % len(matches))\n",
    "    return load_counts(matches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rawcounts_group = f.create_group('RAWCOUNTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "deteff_file = BASEDIR + '/data/det_eff/det_eff.txt'\n",
    "\n",
    "deteff_all = {name: value \n",
    "              for name, value in zip(DETECTORS_ALL, np.loadtxt(deteff_file))}\n",
    "deteff = np.array([deteff_all[key] for key in DETECTORS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rawcounts = rawcounts_group.create_group('GAUSS')\n",
    "for pvec in f['INVECS/GAUSS'].keys():\n",
    "    globpatt = BASEDIR + '/data/singles/summed_sorted/%s_%s.ctx' % (ID, pvec)\n",
    "    try:\n",
    "        counts, fname = vector_to_counts(globpatt)\n",
    "        rawcounts[pvec] = counts\n",
    "        rawcounts[pvec].attrs['FILE'] = fname\n",
    "    except (IOError) as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Loaded data for {} vectors.\".format(len(rawcounts)))\n",
    "print(\"First element has shape {}\".format(next(iter(rawcounts.values())).shape))\n",
    "rawcounts.attrs['DETEFF'] = deteff\n",
    "rawcounts.attrs['DETEFF_FILE'] = deteff_file\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rawcounts = rawcounts_group.create_group('RECR')\n",
    "\n",
    "for pvec in f['INVECS/RECR'].keys():\n",
    "    globpatt = BASEDIR + '/data/singles/summed_sorted/%s_%s.ctx' % (ID, pvec)\n",
    "    try:\n",
    "        counts, fname = vector_to_counts(globpatt)\n",
    "        rawcounts[pvec] = counts\n",
    "        rawcounts[pvec].attrs['FILE'] = fname\n",
    "    except (IOError) as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Loaded data for {} vectors.\".format(len(rawcounts)))\n",
    "print(\"First element has shape {}\".format(next(iter(rawcounts.values())).shape))\n",
    "rawcounts.attrs['DETEFF'] = deteff\n",
    "rawcounts.attrs['DETEFF_FILE'] = deteff_file\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if BASEDIR_OFFSET is not None:\n",
    "    deteff_file = BASEDIR_OFFSET + '/data/det_eff/det_eff.txt'\n",
    "    deteff_all = {name: value \n",
    "                  for name, value in zip(DETECTORS_ALL, np.loadtxt(deteff_file))}\n",
    "    deteff = np.array([deteff_all[key] for key in DETECTORS])\n",
    "    \n",
    "    rawcounts = rawcounts_group.create_group('RECR_OFFSET')\n",
    "\n",
    "    for pvec in f['INVECS/RECR_OFFSET'].keys():\n",
    "        globpatt = BASEDIR_OFFSET + '/data/singles/summed_sorted/%s_%s.ctx' % (ID, pvec)\n",
    "        try:\n",
    "            counts, fname = vector_to_counts(globpatt)\n",
    "            rawcounts[pvec] = counts\n",
    "            rawcounts[pvec].attrs['FILE'] = fname\n",
    "        except (IOError) as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Loaded data for {} vectors.\".format(len(rawcounts)))\n",
    "    print(\"First element has shape {}\".format(next(iter(rawcounts.values())).shape))\n",
    "    rawcounts.attrs['DETEFF'] = deteff\n",
    "    rawcounts.attrs['DETEFF_FILE'] = deteff_file\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reference Data\n",
    "## Single Photon Data\n",
    "Beware: sometimes they are in the wrong order, i.e. singles_1 corresponds to the 5th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_colcounts(col_nr):\n",
    "    globpatt = BASEDIR + '/data/singles/summed_sorted/'\n",
    "    globpatt += '%s_S%i_%.2i.ctx' % (ID, DIM, col_nr)\n",
    "    matches = glob(globpatt)\n",
    "    assert len(matches) == 1, \"It's actually {} for {}\"\\\n",
    "        .format(len(matches), col_nr)\n",
    "    summarydict = parse_ctx(matches[0])\n",
    "    return np.array([summarydict[det] for det in DETECTORS]), matches[0]\n",
    "    \n",
    "single_counts = f.create_group('SINGLE_COUNTS')\n",
    "for n in range(1, len(DETECTORS) + 1):\n",
    "    count, fname = get_colcounts(n)\n",
    "    # carefull! Sometimes they are the wrong way around!\n",
    "    index = n - 1\n",
    "    single_counts[str(index)] = count\n",
    "    single_counts[str(index)].attrs['FILE'] = fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# since they were taken at the same time\n",
    "single_counts.attrs['DETEFF'] = f['RAWCOUNTS/GAUSS'].attrs['DETEFF']\n",
    "single_counts.attrs['DETEFF_FILE'] = f['RAWCOUNTS/GAUSS'].attrs['DETEFF_FILE']\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To check, we plot something proportional to the singles-transfer matrix. Note that we have to transpose counts since the single_counts[i] refer to columns of the transfer matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts = np.array([single_counts[str(i)] for i in range(len(DETECTORS))],\n",
    "                 dtype=np.float64)\n",
    "counts *= single_counts.attrs['DETEFF']\n",
    "img = imshow(np.sqrt(counts).T, interpolation='nearest', show=False)\n",
    "pl.colorbar(img)\n",
    "pl.show()\n",
    "\n",
    "img = imshow(np.abs(target), interpolation='nearest', show=False)\n",
    "pl.colorbar(img)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For convenience we save some of the pre-computed data in the file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DETECTORS = f.attrs['DETECTORS']\n",
    "# Average total photon count (for normalization purposes)\n",
    "tmat_single = np.array([f['SINGLE_COUNTS'][str(i)] for i in range(len(DETECTORS))], dtype=float)\n",
    "deteff = f['SINGLE_COUNTS'].attrs['DETEFF']\n",
    "tmat_single = tmat_single * deteff\n",
    "# axis = 0 since we flip the tmat later\n",
    "tmat_single /= np.max(np.sum(tmat_single, axis=0))\n",
    "tmat_single = np.sqrt(tmat_single.T)\n",
    "f['TMAT_SINGLE'] = tmat_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "deteff = f['RAWCOUNTS/GAUSS'].attrs['DETEFF']\n",
    "group = f.create_group('INTENSITIES')\n",
    "\n",
    "intensities = {}\n",
    "for key, counts in f['RAWCOUNTS']['GAUSS'].items():\n",
    "    counts = counts.value * deteff\n",
    "    intensities[key] = 1.0 * np.mean(counts, axis=0)  # take time-average\n",
    "# we normalize them globally for now, otherwise the solver might have trouble\n",
    "normalization = max(sum(x) for x in intensities.values())\n",
    "intensitiess = {key: x / normalization for key, x in intensities.items()}\n",
    "intensities_group = group.create_group('GAUSSIAN')\n",
    "for key, val in intensities.items():\n",
    "    intensities_group[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "deteff = f['RAWCOUNTS/RECR'].attrs['DETEFF']\n",
    "\n",
    "intensities = {}\n",
    "for key, counts in f['RAWCOUNTS']['RECR'].items():\n",
    "    counts = counts.value * deteff\n",
    "    intensities[key] = 1.0 * np.mean(counts, axis=0)  # take time-average\n",
    "# we normalize them globally for now, otherwise the solver might have trouble\n",
    "normalization = max(sum(x) for x in intensities.values())\n",
    "intensitiess = {key: x / normalization for key, x in intensities.items()}\n",
    "intensities_group = group.create_group('RECR')\n",
    "for key, val in intensities.items():\n",
    "    intensities_group[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if BASEDIR_OFFSET is not None:\n",
    "    deteff = f['RAWCOUNTS/RECR_OFFSET'].attrs['DETEFF']\n",
    "\n",
    "    intensities = {}\n",
    "    for key, counts in f['RAWCOUNTS']['RECR_OFFSET'].items():\n",
    "        counts = counts.value * deteff\n",
    "        intensities[key] = 1.0 * np.mean(counts, axis=0)  # take time-average\n",
    "    # we normalize them globally for now, otherwise the solver might have trouble\n",
    "    normalization = max(sum(x) for x in intensities.values())\n",
    "    intensitiess = {key: x / normalization for key, x in intensities.items()}\n",
    "    intensities_group = group.create_group('RECR_OFFSET')\n",
    "    for key, val in intensities.items():\n",
    "        intensities_group[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Also, load the reconstruction using singles & dips (data missing, fill in in Sec. Dips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    recons = load_complex_array(BASEDIR + '/%s_diprecon.dat' % ID)\n",
    "    f['DIP_RECONSTRUCTED'] = recons\n",
    "    f.flush()\n",
    "    pl.subplot(131)\n",
    "    imshow(recons.real, interpolation='nearest', show=False)\n",
    "    pl.subplot(132)\n",
    "    imshow(recons.imag, interpolation='nearest', show=False)\n",
    "    pl.subplot(133)\n",
    "    imshow(np.abs(recons), interpolation='nearest')\n",
    "    pl.show()\n",
    "\n",
    "    pl.subplot(131)\n",
    "    imshow(target.real, interpolation='nearest', show=False)\n",
    "    pl.subplot(132)\n",
    "    imshow(target.imag, interpolation='nearest', show=False)\n",
    "    pl.subplot(133)\n",
    "    imshow(np.abs(target), interpolation='nearest')\n",
    "    pl.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"Dip reconstruction not found\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
